{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import pandas as pd\n",
    "column_names = [\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'salary'\n",
    "]\n",
    "train_df = pd.read_csv('adult.data', names=column_names)\n",
    "test_df = pd.read_csv('adult.test', names=column_names)\n",
    "train_df = train_df.apply(lambda x: x.str.strip() if x.dtype == 'object' else x)\n",
    "train_df_cp = train_df.copy()\n",
    "train_df_cp = train_df_cp.loc[train_df_cp['native-country'] != 'Holand-Netherlands']\n",
    "train_df_cp.to_csv('train.csv', index=False, header=False)\n",
    "test_df = test_df.apply(lambda x: x.str.strip() if x.dtype == 'object' else x)\n",
    "test_df.to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Predict Adult Salary\").getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"workclass\", StringType(), True),\n",
    "    StructField(\"fnlwgt\", IntegerType(), True),\n",
    "    StructField(\"education\", StringType(), True),\n",
    "    StructField(\"education-num\", IntegerType(), True),\n",
    "    StructField(\"marital-status\", StringType(), True),\n",
    "    StructField(\"occupation\", StringType(), True),\n",
    "    StructField(\"relationship\", StringType(), True),\n",
    "    StructField(\"race\", StringType(), True),\n",
    "    StructField(\"sex\", StringType(), True),\n",
    "    StructField(\"capital-gain\", IntegerType(), True),\n",
    "    StructField(\"capital-loss\", IntegerType(), True),\n",
    "    StructField(\"hours-per-week\", IntegerType(), True),\n",
    "    StructField(\"native-country\", StringType(), True),\n",
    "    StructField(\"salary\", StringType(), True)\n",
    "])\n",
    "\n",
    "train_df = spark.read.csv('train.csv', header=False, schema=schema)\n",
    "test_df = spark.read.csv('test.csv', header=False, schema=schema)\n",
    "\n",
    "categorical_variables = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"-index\") for column in categorical_variables]\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "    outputCols=[\"{0}-encoded\".format(indexer.getOutputCol()) for indexer in indexers]\n",
    ")\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=encoder.getOutputCols(),\n",
    "    outputCol=\"categorical-features\"\n",
    ")\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler])\n",
    "train_df = pipeline.fit(train_df).transform(train_df)\n",
    "test_df = pipeline.fit(test_df).transform(test_df)\n",
    "\n",
    "df=train_df.limit(5).toPandas()\n",
    "continuous_variables = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['categorical-features', *continuous_variables],\n",
    "    outputCol='features'\n",
    ")\n",
    "train_df = assembler.transform(train_df)\n",
    "test_df = assembler.transform(test_df)\n",
    "\n",
    "indexer = StringIndexer(inputCol='salary', outputCol='label')\n",
    "train_df = indexer.fit(train_df).transform(train_df)\n",
    "test_df = indexer.fit(test_df).transform(test_df)\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "model = lr.fit(train_df)\n",
    "\n",
    "pred = model.transform(test_df)\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "attrs = sorted((attr[\"idx\"], attr[\"name\"]) for attr in (chain(*pred.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"].values())))\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "gbCvFeatureImportance = pd.DataFrame([(name, model.coefficients[idx]) for idx, name in attrs],columns=['feature_name','feature_importance'])\n",
    "\n",
    "print(gbCvFeatureImportance.sort_values(by=['feature_importance'],ascending =False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o1242.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\r\n\tat py4j.Gateway.invoke(Gateway.java:274)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-e6555d7755e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpkl_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'../../../Users/amrit/Documents/Assignment1/finalized_model.sav'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkl_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark-2.4.4-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\spark-2.4.4-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n\u001b[0;32m    331\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m                     format(target_id, \".\", name, value))\n\u001b[0m\u001b[0;32m    333\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             raise Py4JError(\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling o1242.__getstate__. Trace:\npy4j.Py4JException: Method __getstate__([]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\r\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\r\n\tat py4j.Gateway.invoke(Gateway.java:274)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pkl_filename = './trained_model.sav'\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

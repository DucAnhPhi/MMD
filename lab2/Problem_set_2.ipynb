{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Massive Datasets: Solutions to Problem Set 2\n",
    "\n",
    "Duc Anh, Phi 3550091\n",
    "\n",
    "Mustafa, Ibrahim 3284705\n",
    "\n",
    "Amritpal, Kaur 3532964\n",
    "\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angle A-B: 0.13232019670460035\n",
      "angle A-C: 0.1748578990699256\n",
      "angle B-C: 0.28240076879579834\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha = 1\n",
    "beta = 1\n",
    "A = np.array([3.06, alpha*500, beta*6])\n",
    "B = np.array([2.68, alpha*320, beta*4])\n",
    "C = np.array([2.92, alpha*640, beta*6])\n",
    "\n",
    "res = np.arccos(np.dot(A, B)/(np.linalg.norm(A)*(np.linalg.norm(B))))*(180/np.pi)\n",
    "print(\"angle A-B:\", res)\n",
    "\n",
    "res = np.arccos(np.dot(A, C)/(np.linalg.norm(A)*(np.linalg.norm(C))))*(180/np.pi)\n",
    "print(\"angle A-C:\", res)\n",
    "\n",
    "res = np.arccos(np.dot(B, C)/(np.linalg.norm(B)*(np.linalg.norm(C))))*(180/np.pi)\n",
    "print(\"angle B-C:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angle A-B: 7.7433676747857705\n",
      "angle A-C: 7.451623812439634\n",
      "angle B-C: 14.262328442796376\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "beta = 0.5\n",
    "A = np.array([3.06, alpha*500, beta*6])\n",
    "B = np.array([2.68, alpha*320, beta*4])\n",
    "C = np.array([2.92, alpha*640, beta*6])\n",
    "\n",
    "res = np.arccos(np.dot(A, B)/(np.linalg.norm(A)*(np.linalg.norm(B))))*(180/np.pi)\n",
    "print(\"angle A-B:\", res)\n",
    "\n",
    "res = np.arccos(np.dot(A, C)/(np.linalg.norm(A)*(np.linalg.norm(C))))*(180/np.pi)\n",
    "print(\"angle A-C:\", res)\n",
    "\n",
    "res = np.arccos(np.dot(B, C)/(np.linalg.norm(B)*(np.linalg.norm(C))))*(180/np.pi)\n",
    "print(\"angle B-C:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.002054794520547945\n",
      "beta: 0.1875\n",
      "angle A-B: 6.071648921036559\n",
      "angle A-C: 5.368226522288894\n",
      "angle B-C: 10.812325507058954\n"
     ]
    }
   ],
   "source": [
    "alpha = 1/np.average(np.array([500,320,640]))\n",
    "beta = 1/np.average(np.array([6,4,6]))\n",
    "print(\"alpha:\", alpha)\n",
    "print(\"beta:\", beta)\n",
    "A = np.array([3.06, alpha*500, beta*6])\n",
    "B = np.array([2.68, alpha*320, beta*4])\n",
    "C = np.array([2.92, alpha*640, beta*6])\n",
    "\n",
    "res = np.arccos(np.dot(A, B)/(np.linalg.norm(A)*(np.linalg.norm(B))))*(180/np.pi)\n",
    "print(\"angle A-B:\", res)\n",
    "\n",
    "res = np.arccos(np.dot(A, C)/(np.linalg.norm(A)*(np.linalg.norm(C))))*(180/np.pi)\n",
    "print(\"angle A-C:\", res)\n",
    "\n",
    "res = np.arccos(np.dot(B, C)/(np.linalg.norm(B)*(np.linalg.norm(C))))*(180/np.pi)\n",
    "print(\"angle B-C:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "### How can a competitor - in principle - try to steal the valuable data for recommendations on this website?\n",
    "\n",
    "The attacker could create fake accounts which like certain items where the competitor does not have much data about. The given recommendations can be then collected and possibly exploited.\n",
    "\n",
    "### Does this work better when the web shop implemented a content-based or a collaborative filtering system?\n",
    "\n",
    "Content-based systems are problematic for attackers, as they are not based on other user data. So the recommdations are only based on attackers input, which is not useful. Collaborative filtering systems (user-user) is preferred as their recommendations are based on other users' ratings.\n",
    "\n",
    "### What data would the competitor be able to infer?\n",
    "\n",
    "The competitor cannot infer any further data from the recommendations. He cannot infer which features the similar products share aswell as the ratings by the similar users. The whole similarity matrix is unknown.\n",
    "\n",
    "### Would this technique have an impact on the recommendation system, i.e. would this attack create a bias on the data?\n",
    "\n",
    "Only for Collaborative Filtering (user-user) an attack would create a bias on the data. This requires the attacker to create enough accounts with enough ratings. The reason for that is that the recommender system evaluates all similar users, which could be fake accounts.\n",
    "\n",
    "### Why is this attack probably not viable in any case?\n",
    "\n",
    "An attacker does not gain any insight into the user or item data. He will solely be presented with recommendations, which even might be biased and thus not representative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "### a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard_distance A-B: 0.5\n",
      "Jaccard_distance A-C: 0.5\n",
      "Jaccard_distance B-C: 0.5\n"
     ]
    }
   ],
   "source": [
    "A = np.array([4,5,0,5,1,0,3,2])\n",
    "B = np.array([0,3,4,3,1,2,1,0])\n",
    "C = np.array([2,0,1,3,0,4,5,3])\n",
    "\n",
    "A = A.astype(bool).astype(int)\n",
    "B = B.astype(bool).astype(int)\n",
    "C = C.astype(bool).astype(int)\n",
    "\n",
    "res = 1 - np.sum(np.bitwise_and(A,B))/np.sum(np.bitwise_or(A,B))\n",
    "print(\"Jaccard_distance A-B:\", res)\n",
    "\n",
    "res = 1 - np.sum(np.bitwise_and(A,C))/np.sum(np.bitwise_or(A,C))\n",
    "print(\"Jaccard_distance A-C:\", res)\n",
    "\n",
    "res = 1 - np.sum(np.bitwise_and(B,C))/np.sum(np.bitwise_or(B,C))\n",
    "print(\"Jaccard_distance B-C:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_distance A-B: 0.6010407640085653\n",
      "cos_distance A-C: 0.6149186938124421\n",
      "cos_distance B-C: 0.5138701197773616\n"
     ]
    }
   ],
   "source": [
    "A = np.array([4,5,0,5,1,0,3,2])\n",
    "B = np.array([0,3,4,3,1,2,1,0])\n",
    "C = np.array([2,0,1,3,0,4,5,3])\n",
    "\n",
    "res = np.dot(A, B)/(np.linalg.norm(A)*(np.linalg.norm(B)))\n",
    "print(\"cos_distance A-B:\", res)\n",
    "\n",
    "res = np.dot(A, C)/(np.linalg.norm(A)*(np.linalg.norm(C)))\n",
    "print(\"cos_distance A-C:\", res)\n",
    "\n",
    "res = np.dot(B, C)/(np.linalg.norm(B)*(np.linalg.norm(C)))\n",
    "print(\"cos_distance B-C:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard_distance A-B: 0.6\n",
      "Jaccard_distance A-C: 0.6666666666666667\n",
      "Jaccard_distance B-C: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "A_original = np.array([4,5,0,5,1,0,3,2])\n",
    "B_original = np.array([0,3,4,3,1,2,1,0])\n",
    "C_original = np.array([2,0,1,3,0,4,5,3])\n",
    "\n",
    "A[A_original>=3] = 1\n",
    "A[A_original<=2] = 0 \n",
    "B[B_original>=3] = 1\n",
    "B[B_original<=2] = 0 \n",
    "C[C_original>=3] = 1\n",
    "C[C_original<=2] = 0 \n",
    "\n",
    "res = 1 - np.sum(np.bitwise_and(A,B))/np.sum(np.bitwise_or(A,B))\n",
    "print(\"Jaccard_distance A-B:\", res)\n",
    "\n",
    "res = 1 - np.sum(np.bitwise_and(A,C))/np.sum(np.bitwise_or(A,C))\n",
    "print(\"Jaccard_distance A-C:\", res)\n",
    "\n",
    "res = 1 - np.sum(np.bitwise_and(B,C))/np.sum(np.bitwise_or(B,C))\n",
    "print(\"Jaccard_distance B-C:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_distance A-B: 0.5773502691896258\n",
      "cos_distance A-C: 0.5\n",
      "cos_distance B-C: 0.2886751345948129\n"
     ]
    }
   ],
   "source": [
    "res = np.dot(A, B)/(np.linalg.norm(A)*(np.linalg.norm(B)))\n",
    "print(\"cos_distance A-B:\", res)\n",
    "\n",
    "res = np.dot(A, C)/(np.linalg.norm(A)*(np.linalg.norm(C)))\n",
    "print(\"cos_distance A-C:\", res)\n",
    "\n",
    "res = np.dot(B, C)/(np.linalg.norm(B)*(np.linalg.norm(C)))\n",
    "print(\"cos_distance B-C:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([4,5,0,5,1,0,3,2],  dtype=float)\n",
    "B = np.array([0,3,4,3,1,2,1,0],  dtype=float)\n",
    "C = np.array([2,0,1,3,0,4,5,3],  dtype=float)\n",
    "\n",
    "A[A!=0] += -np.average(A)\n",
    "B[B!=0] += -np.average(B)\n",
    "C[C!=0] += -np.average(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos_distance A-B: 0.5465040408511785\n",
      "cos_distance A-C: 0.16340829138365012\n",
      "cos_distance B-C: -0.31256152042413066\n"
     ]
    }
   ],
   "source": [
    "res = np.dot(A, B)/(np.linalg.norm(A)*(np.linalg.norm(B)))\n",
    "print(\"cos_distance A-B:\", res)\n",
    "\n",
    "res = np.dot(A, C)/(np.linalg.norm(A)*(np.linalg.norm(C)))\n",
    "print(\"cos_distance A-C:\", res)\n",
    "\n",
    "res = np.dot(B, C)/(np.linalg.norm(B)*(np.linalg.norm(C)))\n",
    "print(\"cos_distance B-C:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The students are the users and the professors are the items. The users could give a numerical rating or star rating to have a higher resolution than you would have with a simple \"like\" system. Additional buttons could be introduced to indicate something like \"recommend for thesis\" if the professor is particular suited as a supervisor during a thesis. This scenario lends itself more to a collaborative filtering approach, since feature selection in this case is non-inutitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artists are the users and the artworks are the items. The users could give a numerical rating or a star rating, just like in the above scenario. Again, this scenario also lends itself for additional buttons like a \"favorite\" button that can serve as an additional factor for determining particular tastes. In this case a content based approach could be used if one constrained the artworks to for example particular styles etc. which could be used as additional features besides the artist. However, a collaborative filtering approach is most likely better suited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The people on the platform are the users and the items are the users themselves. In this case we already have a \"like\" system for rating. This particular scenario fits naturally into the content based approach because similarity should be calculated based on physical features as well as hobbies/tastes etc. The special thing in this scenario is the hidden profile of the dream partner. In this case the profile used to calculate similarity to items (users) would be a combination of the dream profile and the usual weighted average of the items (users) already rated, to introduce a bias towards the dream profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "### a) We store the artistid aliases as a dictionary, as it is reasonably small. This makes it efficient to update the utility matrix with the correct artist ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.master('local').appName('lab2').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+\n",
      "| userid|artistid|playcount|\n",
      "+-------+--------+---------+\n",
      "|1059637| 1000010|      238|\n",
      "|1059637| 1000049|        1|\n",
      "+-------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49481"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origSchema = StructType([\n",
    "    StructField('userid', IntegerType(), True),\n",
    "    StructField('artistid', IntegerType(), True),\n",
    "    StructField('playcount', IntegerType(), True)\n",
    "])\n",
    "\n",
    "origDF = spark.read.option('delimiter', ' ').schema(origSchema).csv('user_artist_data_small.txt')\n",
    "origDF.show(2)\n",
    "origDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliasDict = {}\n",
    "with open('artist_alias_small.txt') as f:\n",
    "    for line in f:\n",
    "        (key, val) = line.split()\n",
    "        aliasDict[int(key)] = int(val)\n",
    "\n",
    "\n",
    "def get_alias(key):\n",
    "    ret = key\n",
    "    if key in aliasDict.keys():\n",
    "        ret = aliasDict[key]\n",
    "    return ret\n",
    "\n",
    "get_alias_udf = F.udf(get_alias, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+\n",
      "| userid|artistid|playcount|\n",
      "+-------+--------+---------+\n",
      "|1059637| 1000010|      238|\n",
      "|1059637| 1000049|        1|\n",
      "+-------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+--------+---------+\n",
      "| userid|artistid|playcount|\n",
      "+-------+--------+---------+\n",
      "|1059637| 1000010|      238|\n",
      "+-------+--------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correctedDF = origDF.withColumn(\n",
    "    'artistid',\n",
    "    get_alias_udf(origDF['artistid'])\n",
    ")\n",
    "\n",
    "correctedDF.show(2)\n",
    "correctedDF.where(correctedDF.userid == 1059637).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "allArtistsDF = correctedDF.select('artistid').distinct().sort(correctedDF.artistid.asc())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(userid1, userid2):\n",
    "    alldf = correctedDF.alias('alldf')\n",
    "    artistsdf = allArtistsDF.alias('artistsdf')\n",
    "    def get_r(userid):\n",
    "        r = alldf.where(F.col('alldf.userid') == userid)\n",
    "        average = r.select(F.avg(F.col('alldf.playcount'))).collect()[0][0]\n",
    "        # Normalize r by subtracting average\n",
    "        r = r.withColumn('playcount', F.col('playcount') - average).alias('left')\n",
    "        # Format r, such that all artist ids are included\n",
    "        r = r.join(artistsdf, alldf['artistid'] == artistsdf['artistid'], 'right') \\\n",
    "            .select('playcount','artistsdf.artistid') \\\n",
    "            .withColumn(\n",
    "                'playcount',\n",
    "                F.when(\n",
    "                    F.col('playcount').isNull(), 0\n",
    "                ).otherwise(F.col('playcount'))\n",
    "            )\n",
    "        return r\n",
    "            \n",
    "    r1 = get_r(userid1).withColumnRenamed('playcount', 'playcount1')\n",
    "    r1.count()\n",
    "    r2 = get_r(userid2).withColumnRenamed('playcount', 'playcount2')\n",
    "    r2.count()\n",
    "    joined = r1.join(r2, r1.artistid == r2.artistid).withColumn('product', F.col('playcount1') * F.col('playcount2'))\n",
    "    joined = joined.withColumn('playcount1_squared', F.col('playcount1') * F.col('playcount1'))\n",
    "    joined = joined.withColumn('playcount2_squared', F.col('playcount2') * F.col('playcount2'))\n",
    "    dotProduct = joined.select(F.sum(F.col('product'))).collect()[0][0]\n",
    "    magnitudeR1 = np.sqrt(joined.select(F.sum(F.col('playcount1_squared'))).collect()[0][0])\n",
    "    magnitudeR2 = np.sqrt(joined.select(F.sum(F.col('playcount2_squared'))).collect()[0][0])\n",
    "    magnitude = magnitudeR1 * magnitudeR2\n",
    "    cos_dist = dotProduct / magnitude\n",
    "    \n",
    "    return cos_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0030835573432774627\n"
     ]
    }
   ],
   "source": [
    "print(get_sim(1059637, 1046559))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sim_udf = F.udf(get_sim, DoubleType())\n",
    "\n",
    "def get_k_most_similar_users(k, userid, artistid, alldf, artistsdf):\n",
    "    # find all users who listened to artistid\n",
    "    similarUsers = alldf \\\n",
    "        .where(F.col('artistid') == artistid) \\\n",
    "        .where(F.col('userid') != userid)\n",
    "    # compute similarity for each user\n",
    "    computed = similarUsers \\\n",
    "        .withColumn('similarity', get_sim_udf(F.col('userid'), F.lit(userid)))\n",
    "    # select k user with highest similarity\n",
    "    computed = computed.sort(F.col('similarity').desc())\n",
    "    kUser = computed.head(k)\n",
    "    return kUser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1240105, 1240113, 1240132, 6776115, 1030848]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('artist_data_small.txt', 'r') as f:\n",
    "    lines = [next(f) for _ in range(5)]\n",
    "\n",
    "favorites = [int(line.split()[0]) for line in lines]\n",
    "print(favorites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49481\n",
      "49486\n"
     ]
    }
   ],
   "source": [
    "newUserId = 1007;\n",
    "newUserDF = spark.createDataFrame(\n",
    "    [\n",
    "        (newUserId, favorites[0], 10),\n",
    "        (newUserId, favorites[1], 1),\n",
    "        (newUserId, favorites[2], 4),\n",
    "        (newUserId, favorites[3], 0),\n",
    "        (newUserId, favorites[4], 5)\n",
    "    ],\n",
    "    origSchema\n",
    ")\n",
    "addedDF = correctedDF.union(newUserDF)\n",
    "print(correctedDF.count())\n",
    "print(addedDF.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating(k, userid, artistid, alldf, artistsdf):\n",
    "    kUser = get_k_most_similar_users(k, userid, artistid, alldf, artistsdf)\n",
    "    kUser = kUser.withColumn('rating', F.col('similarity') * F.col('playcount'))\n",
    "    sumRating = kUser.select(F.sum(F.col('rating'))).collect()[0][0]\n",
    "    sumSimilarity = KUser.select(F.sum(F.col('similarity'))).collect()[0][0]\n",
    "    predictedRating = sumRating / sumSimilarity\n",
    "    return predictedRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: DataFrame[userid: int, artistid: int, playcount: int] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-3f830a14c63d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewUserId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6877393\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddedDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallArtistsDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-f9c06f717dc2>\u001b[0m in \u001b[0;36mget_rating\u001b[0;34m(k, userid, artistid, alldf, artistsdf)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martistid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malldf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martistsdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mkUser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_k_most_similar_users\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martistid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malldf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martistsdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mkUser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkUser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'similarity'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'playcount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msumRating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkUser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msumSimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKUser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'similarity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-b4cbf1ebf3df>\u001b[0m in \u001b[0;36mget_k_most_similar_users\u001b[0;34m(k, userid, artistid, alldf, artistsdf)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# compute similarity for each user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcomputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarUsers\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'similarity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_sim_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'userid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malldf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martistsdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# select k user with highest similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcomputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'similarity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mjudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# This function is for improving the online help system in the interactive interpreter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;34m\"For column literals, use 'lit', 'array', 'struct' or 'create_map' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \"function.\".format(col, type(col)))\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: DataFrame[userid: int, artistid: int, playcount: int] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "get_rating(2, newUserId, 6877393, addedDF, allArtistsDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
